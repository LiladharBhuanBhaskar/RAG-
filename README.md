# RAG-
Local RAG (FAISS + Ollama) ‚Äî A local retrieval-augmented generation app that lets you upload documents, embed them using FAISS, and query them via a local LLM.

# Local RAG (FAISS + Ollama) ‚Äî Quickstart

## Overview
This app allows you to upload documents, automatically chunk and embed them (local), store vectors in FAISS, and ask queries answered by a local LLM (Ollama).
This project implements a **Retrieval-Augmented Generation (RAG)** system that allows users to:
- Upload documents (PDFs or text)
- Automatically extract, chunk, and embed content locally
- Store document vectors using **FAISS**
- Ask questions answered by a **local LLM (Ollama)** or other LLMs like OpenAI or Gemini

Built using **FastAPI**, this project runs fully offline and can be deployed locally or via Docker.

## üß© Features
‚úÖ Upload multiple PDF/Text files  
‚úÖ Automatic text extraction, chunking, and embedding  
‚úÖ FAISS-based vector search  
‚úÖ Querying via local or external LLMs (Ollama, OpenAI, Gemini)  
‚úÖ REST API endpoints for upload and query  
‚úÖ Dockerized for easy deployment  
‚úÖ Includes metadata storage and testing support  


## ‚öôÔ∏è Project Structure
app/
‚îú‚îÄ‚îÄ api/
‚îÇ ‚îú‚îÄ‚îÄ upload.py # Handles document uploads
‚îÇ ‚îú‚îÄ‚îÄ query.py # Handles user queries
‚îÇ
‚îú‚îÄ‚îÄ ingest/
‚îÇ ‚îú‚îÄ‚îÄ processor.py # Extracts, chunks, and embeds documents
‚îÇ
‚îú‚îÄ‚îÄ db/
‚îÇ ‚îú‚îÄ‚îÄ vector_store.py # FAISS vector database
‚îÇ ‚îú‚îÄ‚îÄ init_db.py # Initializes FAISS index
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ ‚îú‚îÄ‚îÄ file_parsers.py # PDF/Text parsing helpers
‚îÇ
‚îú‚îÄ‚îÄ main.py # FastAPI main entry point
‚îú‚îÄ‚îÄ config.py # Configuration and API keys
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ tests/ # Unit and integration tests


---

## üß∞ Prerequisites
- Python **3.10+**
- **pip**
- **Git** (optional)
- **Ollama** installed ‚Üí [https://ollama.ai/download](https://ollama.ai/download)

> üí° Alternatively, you can use Docker (see below).

---


## Option A ‚Äî Run locally (recommended for development)

1. Create & activate venv:
   ```bash
   python -m venv .venv
   source .venv/bin/activate   # Linux / macOS
   .venv\\Scripts\\activate    # Windows PowerShell

2Ô∏è‚É£ Install dependencies

      pip install -r requirements.txt

3Ô∏è‚É£ Run the FastAPI server

      uvicorn app.main:app --reload

üê≥Option B ‚Äî Run with Docker

Make sure Docker is installed and running.

docker-compose up --build

![image alt](https://github.com/LiladharBhuanBhaskar/RAG-/blob/d0e4f19f36a54a6ab735dd20b18afd73a57819df/WhatsApp%20Image%202025-10-27%20at%2015.35.08_0e38e859.jpg)

2nd 
![image alt](https://github.com/LiladharBhuanBhaskar/RAG-/blob/dc5083b5272cd351ff0feebcb408343d066008c5/WhatsApp%20Image%202025-10-27%20at%2015.35.54_cd2c48ef.jpg)
