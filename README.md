# RAG-
Local RAG (FAISS + Ollama) â€” A local retrieval-augmented generation app that lets you upload documents, embed them using FAISS, and query them via a local LLM.

# Local RAG (FAISS + Ollama) â€” Quickstart

## Overview
This app allows you to upload documents, automatically chunk and embed them (local), store vectors in FAISS, and ask queries answered by a local LLM (Ollama).
This project implements a **Retrieval-Augmented Generation (RAG)** system that allows users to:
- Upload documents (PDFs or text)
- Automatically extract, chunk, and embed content locally
- Store document vectors using **FAISS**
- Ask questions answered by a **local LLM (Ollama)** or other LLMs like OpenAI or Gemini

Built using **FastAPI**, this project runs fully offline and can be deployed locally or via Docker.

## ğŸ§© Features
âœ… Upload multiple PDF/Text files  
âœ… Automatic text extraction, chunking, and embedding  
âœ… FAISS-based vector search  
âœ… Querying via local or external LLMs (Ollama, OpenAI, Gemini)  
âœ… REST API endpoints for upload and query  
âœ… Dockerized for easy deployment  
âœ… Includes metadata storage and testing support  


## âš™ï¸ Project Structure
app/
â”œâ”€â”€ api/
â”‚ â”œâ”€â”€ upload.py # Handles document uploads
â”‚ â”œâ”€â”€ query.py # Handles user queries
â”‚
â”œâ”€â”€ ingest/
â”‚ â”œâ”€â”€ processor.py # Extracts, chunks, and embeds documents
â”‚
â”œâ”€â”€ db/
â”‚ â”œâ”€â”€ vector_store.py # FAISS vector database
â”‚ â”œâ”€â”€ init_db.py # Initializes FAISS index
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ file_parsers.py # PDF/Text parsing helpers
â”‚
â”œâ”€â”€ main.py # FastAPI main entry point
â”œâ”€â”€ config.py # Configuration and API keys
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ tests/ # Unit and integration tests


---

## ğŸ§° Prerequisites
- Python **3.10+**
- **pip**
- **Git** (optional)
- **Ollama** installed â†’ [https://ollama.ai/download](https://ollama.ai/download)

> ğŸ’¡ Alternatively, you can use Docker (see below).

---


## Option A â€” Run locally (recommended for development)

1. Create & activate venv:
   ```bash
   python -m venv .venv
   source .venv/bin/activate   # Linux / macOS
   .venv\\Scripts\\activate    # Windows PowerShell

2ï¸âƒ£ Install dependencies

      pip install -r requirements.txt

3ï¸âƒ£ Run the FastAPI server

      uvicorn app.main:app --reload

ğŸ³Option B â€” Run with Docker

Make sure Docker is installed and running.

docker-compose up --build
